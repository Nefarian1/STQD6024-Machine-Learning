{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nefarian1/STQD6024-Machine-Learning/blob/main/Week11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_xKls4MPu9"
      },
      "source": [
        "## Clustering\n",
        "+ Finding subgroups/clusters within data that are similar.\n",
        "+ To define two or more observations to be similar or different, often a domain-specific consideration must be made based on knowledge of the data being studied.\n",
        "+ Clustering and PCA seek to simplify the data via a small number of summaries.\n",
        "+ But, PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n",
        "+ While clustering looks to find homogeneous subgroups among the observations.\n",
        "+ Applications:\n",
        "    + **market segmentation** - identify subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\n",
        "+ Can cluster observations based on the features, or cluster features based on the observations to discover subgroups among the features.\n",
        "+ The latter can be performed by simply transposing data matrix.\n",
        "\n",
        "### K-Means\n",
        "+ Clustering where you define the number of clusters, K, ahead of time.\n",
        "+ Let $C_1, \\ldots, C_K$ denote sets containing indices of the observations, where:\n",
        "    + $C_1 \\cup C_2 \\cup \\ldots \\cup C_K = \\left\\{ 1, \\ldots, n \\right\\}$.\n",
        "    + $C_k \\cap C_{k^\\prime} = \\emptyset$ for all $k \\neq k^\\prime$\n",
        "    + $i \\in C_k$ means that $i^\\textrm{th}$ observation is in the $k^\\textrm{th}$ cluster.\n",
        "+ Good clustering is one for which the within-cluster variation, $W(C_k)$, is as small as possible,\n",
        "$$\\min_{C_1, \\ldots, C_K}\\left\\{ \\sum_{k=1}^K{W(C_k)} \\right\\}$$\n",
        "Which means we want to partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible.\n",
        "+ There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance.\n",
        "$$W(C_k) = \\frac{1}{|C_k|}\\sum_{i,i^\\prime \\in C_k}\\sum_{j=1}^p{(x_{ij} - x_{i^\\prime j})^2}$$\n",
        "where $|C_k|$ is the number of observations in the $k^\\textrm{th}$ cluster.\n",
        "+ One way to solve this is by this approach:\n",
        "    + Randomly assign each point to a cluster\n",
        "    + Iterate until cluster assignments stop changing:\n",
        "        + For each cluster, compute the cluster centroid (vector of the p feature means for the observations in that cluster).\n",
        "        + All points are then reassigned based on whose centroid is closest (using euclidean distance).\n",
        "        + A new centroid is found by averaging the points in each cluster.\n",
        "        + Process stops after centroids stop moving or some max number of iterations.\n",
        "    + It is important to run the algorithm multiple times from different random initial configurations. Selects the best solution\n",
        "+ Can do initial assignment multiple times and choose clustering assignment with least total variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUBHtTiHMPu_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image('images/pw101.png', width =700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1_huWT1MPu_"
      },
      "outputs": [],
      "source": [
        "Image('images/pw102.png', width =700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "i68alzurMPu_"
      },
      "outputs": [],
      "source": [
        "Image('images/pw103.png', width =700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5DxcunRMPvA"
      },
      "source": [
        "### Lab 1\n",
        "+ The `sklearn` function `Kmeans()` performs K-means clustering in Python.\n",
        "+ We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPVfXHO2MPvA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(123)\n",
        "X = np.random.randn(50,2)\n",
        "X[0:25, 0] = X[0:25, 0] + 3\n",
        "X[0:25, 1] = X[0:25, 1] - 4\n",
        "\n",
        "f, ax = plt.subplots(figsize=(6, 5))\n",
        "ax.scatter(X[:,0], X[:,1], s=50)\n",
        "ax.set_xlabel('X0')\n",
        "ax.set_ylabel('X1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6HIFQ2-MPvA"
      },
      "outputs": [],
      "source": [
        "## perform K-means clustering with K = 2:\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters = 2, random_state = 123).fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uegJLo2dMPvA"
      },
      "outputs": [],
      "source": [
        "## The cluster assignments of the 50 observations are contained in kmeans.labels_:\n",
        "print(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lmnsxwIMPvB"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGd5GsK_MPvB"
      },
      "source": [
        "+ The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to `Kmeans()`.\n",
        "+ We can plot the data, with each observation colored according to its cluster assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UWpAwYvMPvB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(X[:,0], X[:,1], s = 50, c = kmeans.labels_, cmap = plt.cm.bwr)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0],\n",
        "            kmeans.cluster_centers_[:, 1],\n",
        "            marker = '*',\n",
        "            s = 150,\n",
        "            color = 'cyan',\n",
        "            label = 'Centers')\n",
        "plt.legend(loc = 'best')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5IInzxvMPvB"
      },
      "source": [
        "+ Here the observations can be easily plotted because they are two-dimensional.\n",
        "+ If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\n",
        "\n",
        "+ In this example, we knew that there really were two clusters because we generated the data.\n",
        "+ However, for real data, in general we do not know the true number of clusters.\n",
        "+ We could instead have performed K-means clustering on this example with `K  =  3`.\n",
        "+ If we do this, K-means clustering will split up the two \"real\" clusters, since it has no information about them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yKvXabhMPvB"
      },
      "outputs": [],
      "source": [
        "kmeans_3_clusters = KMeans(n_clusters = 3, random_state = 123)\n",
        "kmeans_3_clusters.fit(X)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(X[:,0], X[:,1], s=50, c=kmeans_3_clusters.labels_, cmap=plt.cm.prism)\n",
        "plt.scatter(kmeans_3_clusters.cluster_centers_[:, 0], kmeans_3_clusters.cluster_centers_[:, 1], marker='*', s=150,\n",
        "            color='blue', label='Centers')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MDsDnuNMPvB"
      },
      "source": [
        "+ To run the `Kmeans()` function in python with multiple initial cluster assignments, we use the `n_init` argument (default: 10).\n",
        "+ If a value of `n_init` greater than one is used, then K-means clustering will be performed using multiple random assignments, and the `Kmeans()` function will report only the best results.\n",
        "+ Here we compare using `n_init = 1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_PGFWyVMPvB"
      },
      "outputs": [],
      "source": [
        "km_out_single_run = KMeans(n_clusters = 3, n_init = 1, random_state = 123).fit(X)\n",
        "km_out_single_run.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHFi-LCQMPvB"
      },
      "outputs": [],
      "source": [
        "## `n_init = 20`:\n",
        "km_out_single_run = KMeans(n_clusters = 3, n_init = 20, random_state = 123).fit(X)\n",
        "km_out_single_run.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMDpcvK7MPvB"
      },
      "source": [
        "+ Note that `.inertia_` is the total within-cluster sum of squares, which we seek to minimize by performing K-means clustering.\n",
        "\n",
        "+ It is generally recommended to always run K-means clustering with a large value of `n_init`, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.\n",
        "\n",
        "+ When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the `random_state` parameter.\n",
        "+ This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VKMJtA7eMPvB"
      },
      "source": [
        "#### Advantages of K-Means\n",
        "+ Relatively scalable and efficient in processing large data sets\n",
        "+ The computational complexity of the algorithm is $O(nkt)$\n",
        "    + $n$ : total number of objects\n",
        "    + $k$ : number of clusters\n",
        "    + $t$ : number of iterations\n",
        "    + Normally, $k<<n$ and $t<<n$\n",
        "\n",
        "#### Disadvantages of K-Means\n",
        "+ Can be applied only when the mean of a cluster is defined\n",
        "+ Users need to specify $k$\n",
        "+ K-means is not suitable for discovering clusters with nonconvex shapes or clusters of very different size\n",
        "+ It is sensitive to noise and outlier data points (can influence the mean value)\n",
        "\n",
        "#### Notes:\n",
        "+ There were variants of k-means which differ in:\n",
        "    + Selection of the initial k means\n",
        "    + Dissimilarity calculations\n",
        "    + Strategies to calculate cluster means\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnQUKS7zMPvC"
      },
      "source": [
        "### K-modes\n",
        "+ To handle categorical data\n",
        "+ Using modes instead of means\n",
        "+ There were various dissimilarity measures to deal with categorical objects\n",
        "+ Using a frequency based method to update modes of clusters\n",
        "+ The entire algorithm for K-modes is built upon using the highest frequency to form the clusters.\n",
        "+ Mixture of categorical and numerical data.\n",
        "\n",
        "\n",
        "#### Algorithm\n",
        "1. Randomly assign $K$ number of modes. (select initial $k$ number of random data points as modes).\n",
        "2. Calculate the dissimilarity score between each of the remaining data points from the $K$ number of chosen modes.\n",
        "3. Associate the data points to the mode whose score is minimum. (you will have $K$ number of clusters)\n",
        "4. Use ‘Moving mode frequency based method’ to update the modes (for each of the $k$ clusters we need to update the modes).\n",
        "5. Repeat from step 2 until there is no reassignment of clusters or when cost function is minimized.\n",
        "\n",
        "#### Dissimilarity score\n",
        "Let $X$ and $Y$ are two categorical objects having $n$ attributes:\n",
        "$X = [X_1, X_2, \\cdots, X_n]$, $Y = [Y_1, Y_2, \\cdots, Y_n]$\n",
        "The dissimilarity score is given as\n",
        "\n",
        "$$\\textrm{Diss}(X,Y) = \\sum_{j=1}^n{\\delta(X_j,Y_j)}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\delta(X_j,Y_j) = \\begin{cases} 0 \\textrm{ if } X_j=Y_j \\\\\n",
        "1 \\textrm{ if } X_j\\neq Y_j \\end{cases}$$\n",
        "\n",
        "#### Moving mode frequency based method\n",
        "Let $A = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\end{matrix} \\right], B = \\left[ \\begin{matrix} 0 \\\\ 1 \\\\ 1 \\end{matrix} \\right], C = \\left[ \\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix} \\right]$ be three categorical objects having three attributes in binary format.\n",
        "\n",
        "Then the new updated mode will be $\\textrm{Mode} = \\left[ \\begin{matrix} 0 \\\\ 1 \\\\ 1 \\end{matrix} \\right]$\n",
        "\n",
        "#### Cost function\n",
        "Let there be $C_i (i=1,\\ldots,K)$ number of clusters formed after using k-modes algorithm, then the cost function is given by\n",
        "\n",
        "$$J=\\sum_{i=1}^k{\\sum_{X_j \\in C_i}{\\textrm{Diss}(X_j,Q_i)}}$$\n",
        "\n",
        "where $X_j$ is categorical object of $i^\\textrm{th}$ cluster and $Q_i$ is the mode of the $i^\\textrm{th}$ cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jxUrpYsMPvC"
      },
      "source": [
        "### Lab 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkVF8NTaMPvC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from kmodes.kmodes import KModes\n",
        "\n",
        "# random categorical data\n",
        "data = np.random.choice(20, (100, 10))\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eJKMbgAMPvC"
      },
      "outputs": [],
      "source": [
        "km = KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)\n",
        "\n",
        "clusters = km.fit_predict(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2zJlEzQMPvC"
      },
      "outputs": [],
      "source": [
        "# Print the cluster centroids\n",
        "print(km.cluster_centroids_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8217QRERMPvC"
      },
      "outputs": [],
      "source": [
        "km.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21K1T-6fMPvC"
      },
      "source": [
        "### Lab 3\n",
        "\n",
        "download soybean dataset: https://github.com/nicodv/kmodes/tree/master/examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEsDikgeMPvC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from kmodes.kmodes import KModes\n",
        "\n",
        "# reproduce results on small soybean data set\n",
        "x = np.genfromtxt('data/soybean.csv', dtype=int, delimiter=',')[:, :-1]\n",
        "y = np.genfromtxt('data/soybean.csv', dtype=str, delimiter=',', usecols=(35, ))\n",
        "\n",
        "kmodes_huang = KModes(n_clusters=4, init='Huang', verbose=1)\n",
        "kmodes_huang.fit(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjumb_gtMPvC"
      },
      "outputs": [],
      "source": [
        "# Print cluster centroids of the trained model.\n",
        "print('k-modes (Huang) centroids:')\n",
        "print(kmodes_huang.cluster_centroids_)\n",
        "# Print training statistics\n",
        "print('Final training cost: {}'.format(kmodes_huang.cost_))\n",
        "print('Training iterations: {}'.format(kmodes_huang.n_iter_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDhg7kSIMPvC"
      },
      "outputs": [],
      "source": [
        "kmodes_cao = KModes(n_clusters=4, init='Cao', verbose=1)\n",
        "kmodes_cao.fit(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoo6D-P5MPvC"
      },
      "outputs": [],
      "source": [
        "# Print cluster centroids of the trained model.\n",
        "print('k-modes (Cao) centroids:')\n",
        "print(kmodes_cao.cluster_centroids_)\n",
        "# Print training statistics\n",
        "print('Final training cost: {}'.format(kmodes_cao.cost_))\n",
        "print('Training iterations: {}'.format(kmodes_cao.n_iter_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVWhIPRLMPvD"
      },
      "outputs": [],
      "source": [
        "print('Results tables:')\n",
        "for result in (kmodes_huang, kmodes_cao):\n",
        "    classtable = np.zeros((4, 4), dtype=int)\n",
        "    for ii, _ in enumerate(y):\n",
        "        classtable[int(y[ii][-1]) - 1, result.labels_[ii]] += 1\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"    | Cl. 1 | Cl. 2 | Cl. 3 | Cl. 4 |\")\n",
        "    print(\"----|-------|-------|-------|-------|\")\n",
        "    for ii in range(4):\n",
        "        prargs = tuple([ii + 1] + list(classtable[ii, :]))\n",
        "        print(\" D{0} |    {1:>2} |    {2:>2} |    {3:>2} |    {4:>2} |\".format(*prargs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dwJbSP3MPvD"
      },
      "source": [
        "## K-Medoids\n",
        "+ Minimize sensitivity of k-means to outliers\n",
        "+ Pick actual objects to represent clusters instead of mean values\n",
        "+ Each remaining object is clustered with the representative object (called medoids) to which is the most similar\n",
        "+ The algorithm minimizes the sum of the dissimilarities between each object and its corresponding reference point, where the absolute-error criterion is defined as\n",
        "$$ E = \\sum_{j=1}^k{\\sum_{p \\in C_j}\\left|p-o_j\\right|}$$\n",
        "where $p$ is the point in space representing a given object in cluster $C_j$ and $o_j$ is the representative object of $C_j$.\n",
        "+ The algorithm iterates until, eventually, each representative object is actually the medoid, or most centrally located object, of its cluster. This is the basis of the k-medoids method for grouping $n$ objects into $k$ clusters.\n",
        "\n",
        "### Partitioning Around Medoids (PAM)\n",
        "1. After an initial random selection of $k$ representative objects, the algorithm repeatedly tries to make a better choice of cluster representatives.\n",
        "2. Randomly select a non-representative object\n",
        "3. All of the possible pairs of objects are analyzed, where one object in each pair is considered a representative object and the other is not.\n",
        "4. The quality of the resulting clustering is calculated for each such combination.\n",
        "5. An object, $o_j$, is replaced with the object causing the greatest reduction in error.\n",
        "6. The set of best objects for each cluster in one iteration forms the representative objects for the next iteration.\n",
        "7. The final set of representative objects are the respective medoids of the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjNejIG5MPvD"
      },
      "outputs": [],
      "source": [
        "Image('images/pw1012.png', width =900)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20PGuOhMPvD"
      },
      "source": [
        "!pip install pyclustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJESqqUcMPvD"
      },
      "outputs": [],
      "source": [
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from pyclustering.cluster import cluster_visualizer\n",
        "from pyclustering.utils import read_sample\n",
        "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
        "# Load list of points for cluster analysis.\n",
        "sample = read_sample(FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS)\n",
        "# Set random initial medoids.\n",
        "initial_medoids = [1, 500]\n",
        "# Create instance of K-Medoids algorithm.\n",
        "kmedoids_instance = kmedoids(sample, initial_medoids)\n",
        "# Run cluster analysis and obtain results.\n",
        "kmedoids_instance.process()\n",
        "clusters = kmedoids_instance.get_clusters()\n",
        "# Show allocated clusters.\n",
        "print(clusters)\n",
        "# Display clusters.\n",
        "visualizer = cluster_visualizer()\n",
        "visualizer.append_clusters(clusters, sample)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr9kc7SNMPvD"
      },
      "outputs": [],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9BWLP8-MPvD"
      },
      "outputs": [],
      "source": [
        "from pyclustering.samples.definitions import SIMPLE_SAMPLES\n",
        "from pyclustering.cluster import cluster_visualizer\n",
        "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from pyclustering.utils import read_sample\n",
        "\n",
        "import numpy, warnings\n",
        "numpy.warnings = warnings\n",
        "\n",
        "# load list of points for cluster analysis\n",
        "sample = read_sample(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)\n",
        "\n",
        "# initialize\n",
        "initial_medoids = kmeans_plusplus_initializer(sample, 4, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize(return_index=True)\n",
        "\n",
        "# create instance of K-Medoids algorithm\n",
        "kmedoids_instance = kmedoids(sample, initial_medoids)\n",
        "\n",
        "# run cluster analysis and obtain results\n",
        "kmedoids_instance.process()\n",
        "clusters = kmedoids_instance.get_clusters()\n",
        "medoids = kmedoids_instance.get_medoids()\n",
        "\n",
        "# visualize clustering results\n",
        "visualizer = cluster_visualizer(1)\n",
        "visualizer.append_clusters(clusters, sample, 0)\n",
        "visualizer.append_cluster([sample[index] for index in initial_medoids], marker='*', markersize=5)\n",
        "visualizer.append_cluster(medoids, data=sample, marker='*', markersize=15)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIgJe1KfMPvD"
      },
      "outputs": [],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OHbjKcycMPvD"
      },
      "source": [
        "## K-Prototypes\n",
        "Mixture of continuous and categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VVaAAp5MPvD"
      },
      "outputs": [],
      "source": [
        "#create dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "operating_systems = [\"Android\",\"iOS\"]\n",
        "isp_names = [\"Cox\",\"HughesNet\",\"Xfinity\",\"AT&T\"]\n",
        "\n",
        "data = []\n",
        "for i in range(100):\n",
        "    row = []\n",
        "    row.append(np.random.choice(operating_systems)) #OS\n",
        "    row.append(np.random.choice(isp_names)) #ISP\n",
        "    row.append(np.random.poisson(lam=25)) #Age\n",
        "    row.append(np.random.uniform(low=0.5, high=1000)) #Time Spent\n",
        "    data.append(row)\n",
        "\n",
        "customers = pd.DataFrame(data, columns = ['OS', 'ISP','Age','Time Spent'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goswE8LvMPvJ"
      },
      "outputs": [],
      "source": [
        "customers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjIOyo1IMPvJ"
      },
      "outputs": [],
      "source": [
        "#make sure to rescale those continuous variables\n",
        "\n",
        "from sklearn import preprocessing\n",
        "customers_norm = customers.copy()\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "customers_norm[['Age','Time Spent']] = scaler.fit_transform(customers_norm[['Age','Time Spent']])\n",
        "customers_norm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpPLYG3IMPvJ"
      },
      "outputs": [],
      "source": [
        "!pip install kmodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4xxCrs4MPvK"
      },
      "outputs": [],
      "source": [
        "from kmodes.kprototypes import KPrototypes\n",
        "kproto = KPrototypes(n_clusters=3, init='Cao')\n",
        "\n",
        "# specify which columns in the data are categorical (columns 0 and 1)\n",
        "clusters = kproto.fit_predict(customers_norm, categorical=[0, 1])\n",
        "#join data with labels\n",
        "labels = pd.DataFrame(clusters)\n",
        "labeledCustomers = pd.concat((customers,labels),axis=1)\n",
        "labeledCustomers = labeledCustomers.rename({0:'labels'},axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV8WSMjDMPvK"
      },
      "outputs": [],
      "source": [
        "labeledCustomers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unXAm2iKMPvK"
      },
      "outputs": [],
      "source": [
        "labeledCustomers.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5TuXeKdMPvK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL15cPypMPvK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "\n",
        "ax.scatter(labeledCustomers.OS.astype('category').cat.codes, labeledCustomers.ISP.astype('category').cat.codes,\n",
        "           labeledCustomers.Age, c = labeledCustomers.labels)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVLFa_lPMPvK"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "ax.scatter(labeledCustomers.ISP.astype('category').cat.codes, labeledCustomers['Time Spent'],\n",
        "           labeledCustomers.Age, c = labeledCustomers.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG-uhZhcMPvK"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "ax.scatter(labeledCustomers.OS.astype('category').cat.codes, labeledCustomers['Time Spent'],\n",
        "           labeledCustomers.Age, c = labeledCustomers.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-UHrs_jMPvK"
      },
      "source": [
        "# Common distance measures used for clustering\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNcz5ii_MPvK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}