{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nefarian1/STQD6024-Machine-Learning/blob/main/Week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04GLpVzke80"
      },
      "source": [
        "# Chapter 4 - Classification\n",
        "This chapter will discuss qualitative (categorical) variables (classification problems). Classification problems can be thought of as regression problems since most of the models return a probability of being in a certain class. Since probabilities take on real values the problem can also be called regression - as in logistic regression - a machine learning technique that is used for classification but returns probabilities as real values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWZS5Zqmke81"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_YsnEbuke82"
      },
      "outputs": [],
      "source": [
        "default = pd.read_csv(\"data/default.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf_ig-KBke82"
      },
      "outputs": [],
      "source": [
        "default.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCpSidWeke82"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(x='balance', y='income', data=default.sample(1000), hue='default', fit_reg=False)\n",
        "plt.ylim([0,70000])\n",
        "plt.xlim([-100,2500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJJnImOke82"
      },
      "outputs": [],
      "source": [
        "# evidence tha\n",
        "sns.boxplot(x='student', y='balance', data=default, hue='default');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BGgx6qDke83"
      },
      "source": [
        "# Why not linear regression\n",
        "Though it is always possible to use numeric values for the categories of the response, there generally is no natural way to order and separate the values in a way that makes sense. Only in a two-category problem will the ordering make sense but even then linear regression will produce probability estimates outside of [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_z261DWke83"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from scipy import stats\n",
        "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qr_Pnf2ke83"
      },
      "outputs": [],
      "source": [
        "# make column for Yes defaults\n",
        "default['Yes'] = (default['default'] == 'Yes').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h_jZ6YCke84"
      },
      "outputs": [],
      "source": [
        "# linear regression model for default\n",
        "# not ideal and could be made better by simply making all probabilites < 0 equal to 0\n",
        "# and all probabilites > 1 equal to 1\n",
        "sns.regplot(x='balance', y='Yes', data=default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v14morcGke84"
      },
      "source": [
        "# Logistic regression\n",
        "Find function that always outputs number between 0 and 1. Many functions satisfy this condition. For logistic regression the ... logistic function! is used.\n",
        "\n",
        "$$y = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$$\n",
        "\n",
        "Many times you will see this as the sigmoid function in a simpler format\n",
        "\n",
        "$$y = \\frac{1}{1 + e^{-t}}$$\n",
        "\n",
        "Where $t$ is just the normal linear model $t = \\beta_0 + \\beta_1X$. Some algebra can be used to show the two equations above are equivalent.\n",
        "\n",
        "y can now be thought as the probability given some value X since it will always be between 0 and 1. Some more algebra can show that $$log{\\frac{p(X)}{1 - p(X)}} = \\beta_0 + \\beta_1X$$\n",
        "\n",
        "Where $y$ has been replaced by $p(X)$, the probability of $X$. The expression $\\frac{p(X)}{1 - p(X)}$ is known as the 'odds'. So for instance if you are gambling and think that Clinton will win the presidency 80% of the time. The odds would be .8/.2 = 4 or said \"4 to 1\". For every 4 times she wins, Trump will win once.\n",
        "\n",
        "What logistic regression is saying, that the log odds are modeled by a linear model which can be solved by linear regression. This has the literal meaning of - given a one unit increase in one of the variables (say $X_1$), a $\\beta_1$ increase will occur to the log-odds. Or equivalently, the odds will be multiplied by $e^{\\beta_1}$.\n",
        "\n",
        "In our election example, $X_1$ could be the percentage of voters under 30 and $\\beta_1$ could be .5. That would mean if $X_1$ were to increase by 1 percentage point, Clinton's log odds would increase by .5. In our example from above, Clinton's log odds would go from 4 to 4.5 and her probability of winning would go from 80% to 4.5 / 5.5 or 82%\n",
        "\n",
        "There is no straight-line relationship between the probability of being in a certain class and X in logistic regression because of the need to have probabilities between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaxjeKake84"
      },
      "source": [
        "# Estimating coefficients through Maximum Likelihood\n",
        "In linear regression, the model coefficients were found by minimizing the squared residuals. In logistic regression, we maximize the probabilities of all points by a method called maximum likelihood. Maximum likelihood multiplies the model probability for each observation together and chooses parameters that maximize this number. The log likelihood is actually used as numerical underflow will be a problem for must problems with a non-trivial amount of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzdzfhPOke84"
      },
      "outputs": [],
      "source": [
        "# Think about doing a maximum likelihood example with 4 coin tosses and 3 heads.\n",
        "# Start with .5 then choose .6 and see the differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnIZDdSake85"
      },
      "outputs": [],
      "source": [
        "results = smf.logit('Yes ~ balance', data=default).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnspijMAke85"
      },
      "outputs": [],
      "source": [
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRC0KiA_ke85"
      },
      "outputs": [],
      "source": [
        "# Looks quite a bit different than the linear regression model\n",
        "sns.lmplot(x='balance', y='Yes', data=default, logistic=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us4NQwj2ke85"
      },
      "source": [
        "## Interpretation\n",
        "For every one dollar increase in balance the log odds increases by .0555. The log odds when there is no balance is -10.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_IzskC8ke85"
      },
      "outputs": [],
      "source": [
        "# p / (1 - p) =\n",
        "odds = np.exp(-10.65)\n",
        "odds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOGeoUNTke85"
      },
      "outputs": [],
      "source": [
        "# (1 - p) / p = 1 / odds\n",
        "one_over_odds = 1 / odds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxQ0z4pUke85"
      },
      "outputs": [],
      "source": [
        "# 1/p = one_over_odds + 1\n",
        "one_over_p = one_over_odds + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-C0czwwke85"
      },
      "outputs": [],
      "source": [
        "# p = 1 / one_over_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms232Sizke85"
      },
      "outputs": [],
      "source": [
        "# since p is so close to 0, p / (1 - p) is nearly equivalent to p\n",
        "p = 1 / one_over_p\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5eXHmBfke85"
      },
      "source": [
        "# Scikit-learn for easier prediction\n",
        "http://scikit-learn.org/stable/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKUS8Vxdke86"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cK6HmUJke86"
      },
      "outputs": [],
      "source": [
        "# this actually uses regularization by default which will be covered in later chapters\n",
        "lr = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rowiaUJLke86"
      },
      "outputs": [],
      "source": [
        "X = np.column_stack((np.ones(len(default)), default['balance']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIBhxKPqke86"
      },
      "outputs": [],
      "source": [
        "lr.fit(X, default['Yes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwN2--Xuke86"
      },
      "outputs": [],
      "source": [
        "# Model coefficients are different by B1 is very similar\n",
        "lr.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCOQZOTlke86"
      },
      "outputs": [],
      "source": [
        "# predict 1000 dollar balance default\n",
        "lr.predict_proba([[1, 1000]]) # 99 percent chance no default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVfptdq8ke86"
      },
      "outputs": [],
      "source": [
        "# predict 2000 dollar balance default\n",
        "lr.predict_proba([[1, 2000]]) ## 55 percent chance default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh1RkRn2ke86"
      },
      "outputs": [],
      "source": [
        "# predict 3000 dollar balance default\n",
        "lr.predict_proba([[1, 3000]]) ## >99 percent chance default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLOjqS4Gke86"
      },
      "source": [
        "# Multiple Linear Regression\n",
        "more than 1 predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2P83N85ke86"
      },
      "outputs": [],
      "source": [
        "results = smf.logit('Yes ~ balance + student', data=default).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr0k3_KZke87"
      },
      "outputs": [],
      "source": [
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXNY-VLEke87"
      },
      "outputs": [],
      "source": [
        "default['student_yes'] = (default['student'] == 'Yes').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VPa2RLtke87"
      },
      "outputs": [],
      "source": [
        "X = np.column_stack((np.ones(len(default)), default['balance'], default['student_yes']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8fwnIXxke87"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWX2YsnSke87"
      },
      "outputs": [],
      "source": [
        "lr.fit(X, default['Yes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAh_I_lPke87"
      },
      "outputs": [],
      "source": [
        "lr.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdPERqyNke88"
      },
      "outputs": [],
      "source": [
        "# predict 1000 dollar balance default\n",
        "lr.predict_proba([[1, 2000, 1]]) # 99 percent chance no default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9fKA1_kke88"
      },
      "source": [
        "# Simpsons paradox\n",
        "\n",
        "A phenomenon in probability and statistics, in which a trend appears in several different groups of data but disappears or reverses when these groups are combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wNNPPuke88"
      },
      "outputs": [],
      "source": [
        "results = smf.logit('Yes ~ student', data=default).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r_W2ti0ke88"
      },
      "source": [
        "The first model above with both balance and student show a negative relationship between student and default - meaning that being a student decreases the likelihood of defaulting. The second model shows the opposite, that being a student increases the chance at defaulting. This can be explained by the fact that students have more debt on average but compared to those with the same amount of debt they are less likely to default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBfTcT1ke88"
      },
      "source": [
        "# Linear Discriminant Analysis\n",
        "Not to be confused with latent dirichlet allocation. Used for multiclass classification problems.   LDA assumes all predictor variables come from a gaussian distribution and estimates the mean and variance for each predictor variable where the variance is the same across for each predictor variable. It also estimates a prior probability simply by using the proporton of classes in the training set.\n",
        "\n",
        "Bayes rule is used to compute a probability for each class. When there is more than one predictor, a multivariate gaussian is used. Correlations between each predictor must be estimated (the covariance matrix) as they are a parameter to the multivariate gaussian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N92aWIZCke88"
      },
      "source": [
        "+ Suppose there are $K \\geq 2$ classes.\n",
        "\n",
        "+ Let $\\pi_k$ be the overall/prior probability that a randomly chosen observation comes from the $k$th class.\n",
        "\n",
        "+ Let  $\\, f_k(x) \\equiv P(X=x \\,|\\,  Y=k)$  denote the density function of $X$ for an obs that comes from the $k$th class.\n",
        "\n",
        "Then **Bayes theorem** states that\n",
        "$$P(Y=k \\, | \\, X=x ) = \\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^K{\\pi_l f_l(x)}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Nr1MKGke88"
      },
      "source": [
        "Here we will use $p_k(x) = P(Y=k \\, | \\, X=x)$ which is the posterior probability that an observation\n",
        "$X = x$ belongs to the $k$th class.\n",
        "\n",
        "+ Estimating $\\pi_k$ is easy if we have a random sample of $Y$ from the population.\n",
        "\n",
        "+ Estimating $f_k(x)$ is challenging, hence we assume some simple forms for these densities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeTiym7gke88"
      },
      "source": [
        "### LDA for one predictor, $p=1$\n",
        "\n",
        "+ Assume that $f_k(x)$ is normal/Gaussian,\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi}\\sigma_k} \\exp \\left( -\\frac{1}{2 \\sigma_k^2} (x - \\mu_k)^2\\right)$$\n",
        " where $\\mu_k$ and $\\sigma^2_k$ are the mean and variance for the $k$th class.\n",
        "\n",
        "+ Assume for now, $\\sigma^2_1 = \\ldots = \\sigma^2_K = \\sigma^2$.\n",
        "\n",
        "+ Hence,\n",
        "$$p_k(x) = \\dfrac{\\pi_k \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} (x - \\mu_k)^2\\right)}{\\sum_{l=1}^K{\\pi_l \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} (x - \\mu_l)^2\\right)}}$$\n",
        "\n",
        "+ Taking the log and rearranging the terms, this is equivalent to assigning the observation to the class for which\n",
        "$$\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)$$\n",
        "is largest.\n",
        "\n",
        "+ If $K=2$ and $\\pi_1 = \\pi_2$, then the Bayes classifier assigns an observation to class 1 if $$2x(\\mu_1 - \\mu_2) > \\mu_1^2 - \\mu_2^2$$ and to class 2 if otherwise. The Bayes decision boundary is the points where $$x = \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2)} = \\frac{\\mu_1 + \\mu_2}{2}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETLh5Ftike89"
      },
      "source": [
        "#### Example\n",
        "+ 2 classes\n",
        "+ $\\mu_1 = -1.25$, $\\mu_2 = 1.25$ and $\\sigma_1^2 = \\sigma_2^2 = 1$.\n",
        "+ Assume $\\pi_1 = \\pi_2 = 0.5$\n",
        "+ Class 1: $x<0$\n",
        "+ Class 2: $x>0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0VBVNLWke89"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image('images/p1.png', width =400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oir5Vok-ke89"
      },
      "source": [
        "+ Through LDA, the following estimates are used:\n",
        "$$\\begin{aligned}\n",
        "    \\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i = k}{x_i} \\\\\n",
        "    \\hat{\\sigma}^2 &= \\frac{1}{n-K}\\sum_{k=1}^K{\\sum_{i:y_i = k}{(x_i - \\hat{\\mu}_k)^2}}\n",
        "\\end{aligned}$$\n",
        "+ $n$ - total number of training observations\n",
        "+ $n_k$ - number of training observations in the $k$th class\n",
        "+ Without prior knowledge, LDA estimates of $\\pi_k$ are\n",
        "    $$\\hat{\\pi}_k = \\frac{n_k}{n}$$\n",
        "+ Hence, an observation is assign to the class for which the **discriminant function**\n",
        "$$\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log(\\hat{\\pi}_k)$$\n",
        "is largest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpDdt7SJke89"
      },
      "source": [
        "#### Example (cont.)\n",
        "+ 20 observations were drawn from each of the two classes, and are shown as histograms.\n",
        "+ Dashed vertical line - Bayes decision boundary\n",
        "+ Solid vertical line - LDA decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "9ikH8eNjke89"
      },
      "outputs": [],
      "source": [
        "Image('images/p2.png', width =400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3AMsdG9ke89"
      },
      "source": [
        "### LDA for $p>1$\n",
        "+ Assume $X = (X_1, X_2, \\ldots, X_p)$ is drawn from a multivariate Gaussian distribution.\n",
        "+ The density function is defined as\n",
        "$$f(x) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu)^T \\mathbf{\\Sigma}^{-1} (x-\\mu) \\right)$$\n",
        " where $\\mu_k$ is a class-specific mean vector and $\\mathbf{\\Sigma}$ is a covariance matrix that is common to all K classes.\n",
        "+ The Bayes classifier assigns an observation $X = x$ to the class for which\n",
        "$$\\delta_k(x) = x^T \\mathbf{\\Sigma}^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\mathbf{\\Sigma}^{-1}\\mu_k + \\log{\\pi_k}$$\n",
        "is largest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzFpIrUVke89"
      },
      "source": [
        "#### Example\n",
        "+ 3 equally sized Gaussian classes, with class-specific mean vectors and a common covariance matrix.\n",
        "+ Two predictors, $p=2$.\n",
        "+ 20 observations from each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "y2OeRhaqke89"
      },
      "outputs": [],
      "source": [
        "Image('images/p3.png', width =800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4dmoCI6eke89"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
        "y = np.array([1, 1, 1, 2, 2, 2])\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQIy63Ywke8-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyYpZemKke8-"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y, lda.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "l9owrcKake8-"
      },
      "outputs": [],
      "source": [
        "print(lda.predict([[-0.8, -1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cx2kp_bke8-"
      },
      "source": [
        "#### Example: Default Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtTm9X12ke8-"
      },
      "outputs": [],
      "source": [
        "default = pd.read_csv(\"data/default.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjJ1V1MTke8-"
      },
      "outputs": [],
      "source": [
        "default['Yes'] = (default['default'] == 'Yes').astype(int)\n",
        "default['St'] = (default['student'] == 'Yes').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW-jM8vEke8-"
      },
      "outputs": [],
      "source": [
        "X=default[['St', 'balance']].values\n",
        "y=default['Yes'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgLqjkArke8-"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkO2oA2Oke8-"
      },
      "outputs": [],
      "source": [
        "lda.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "pwOB5mG8ke8_"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y,lda.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XezL1O_uke8_"
      },
      "outputs": [],
      "source": [
        "Image('images/p4.png', width =400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NN2ZzjCke8_"
      },
      "source": [
        "##### Confusion matrix\n",
        "+ LDA predicted that a total of 104 people would default.\n",
        "+ 81 actually defaulted and 23 did not.\n",
        "+ Only 23 out of 9667 of the individuals who did not default were incorrectly labeled\n",
        "+ However, of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA.\n",
        "+ Even though the overall error rate is low, the error rate among individuals who defaulted is very high.\n",
        "\n",
        "+ **Sensitivity** is the percentage of true defaulters that are identified (81/333 = 24.3%).\n",
        "+ **Specificity** is the percentage of non-defaulters that are correctly identified (9644/9667 = 99.8%).\n",
        "+ LDA have a low sensitivity and high specificity. This is because LDA try to yield the smallest total error.\n",
        "+ This is because, Bayes classifier assign an observation to the class for which the posterior probability, $p_k(X)$ is greatest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24HQMQike8_"
      },
      "source": [
        "+ For two class case, an observation is assign to the *default* class if\n",
        "$$P(\\textrm{default = Yes} \\, | \\, X=x) >0.5 $$\n",
        "+ The threshold value (0.5) can be lowered or increased (Set by domain expert). For example, if we set\n",
        "$$P(\\textrm{default = Yes} \\, | \\, X=x) >0.2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fksbn5Like8_"
      },
      "outputs": [],
      "source": [
        "lda2 = LinearDiscriminantAnalysis(priors=[0.9,0.1])\n",
        "lda2.fit(X,y)\n",
        "confusion_matrix(y,lda2.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZEzE2mhke8_"
      },
      "outputs": [],
      "source": [
        "lda.priors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DCGMDAsKke8_"
      },
      "outputs": [],
      "source": [
        "Image('images/p5.png', width =400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xIjC5Sa9ke8_"
      },
      "outputs": [],
      "source": [
        "Image('images/p6.png', width =600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EURuG248ke8_"
      },
      "source": [
        "+ black solid line - overall error rate\n",
        "+ blue dashed line - the fraction of defaulting customers that are incorrectly classified\n",
        "+ orange dotted line - the fraction of errors among the non-defaulting customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0ZesCxUke8_"
      },
      "outputs": [],
      "source": [
        "lr.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "xaADaiIJke8_"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y,lr.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiBXhWeAke9A"
      },
      "source": [
        "#### ROC curve\n",
        "+ simultaneously display the two types of errors for all possible thresholds.\n",
        "+ The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC).\n",
        "+ The larger the AUC the better the classifier.\n",
        "+ True positive rate = sensitivity\n",
        "+ False positive rate = (1 - specifictity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aJ0LH2Kke9A"
      },
      "outputs": [],
      "source": [
        "Image('images/p8.png', width =600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31cu8tSke9A"
      },
      "outputs": [],
      "source": [
        "Image('images/p9.png', width =600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "AwTntuBuke9A"
      },
      "outputs": [],
      "source": [
        "Image('images/p7.png', width =400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Ov0W6jke9A"
      },
      "source": [
        "### QDA\n",
        "+ Same as LDA, QDA assumes the observations from each class are drawn from a Gaussian distribution.\n",
        "+ However, QDA assumes that each class has its own covariance matrix.\n",
        "$$\\begin{aligned}\n",
        "\\delta_k(x) &= -\\frac{1}{2} (x-\\mu_k)^T \\mathbf{\\Sigma}_k^{-1} (x-\\mu_k) - \\frac{1}{2} \\log| \\mathbf{\\Sigma}_k| + \\log{\\pi_k} \\\\\n",
        "&= -\\frac{1}{2} x^T \\mathbf{\\Sigma}_k^{-1} x +  x^T \\mathbf{\\Sigma}_k^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\mathbf{\\Sigma}_k^{-1} \\mu_k - \\frac{1}{2} \\log| \\mathbf{\\Sigma}_k| + \\log{\\pi_k}\n",
        "\\end{aligned}$$\n",
        "+ LDA is a much less flexible (lower variance) classifier than QDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVr2JuiFke9A"
      },
      "source": [
        "#### Example\n",
        "+ The Bayes (purple dashed), LDA (black dotted), and QDA (green solid)\n",
        "+ Left: Equal covariances\n",
        "+ Right: Not equal covariances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjCUVsTKke9A"
      },
      "outputs": [],
      "source": [
        "Image('images/p10.png', width =700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw4KhwQ1ke9A"
      },
      "source": [
        "# Stock market prediciton lab\n",
        "This data set consists of:\n",
        "+ percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005.\n",
        "+ For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5.\n",
        "+ We have also recorded :\n",
        "    + Volume (the number of shares traded on the previous day, in billions),\n",
        "    + Today (the percentage return on the date in question)\n",
        "    + Direction (whether the market was Up or Down on this date)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moT8N3dZke9A"
      },
      "outputs": [],
      "source": [
        "smarket = pd.read_csv('data/smarket.csv')\n",
        "smarket.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV0mJW__ke9A"
      },
      "outputs": [],
      "source": [
        "smarket['Up'] = np.where(smarket['Direction'] == 'Up', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXyhZ3Jtke9B"
      },
      "outputs": [],
      "source": [
        "smarket.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ4Y0ctUke9B"
      },
      "outputs": [],
      "source": [
        "X = smarket[['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5']].values\n",
        "y = smarket['Up'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SecZ6pJpke9B"
      },
      "outputs": [],
      "source": [
        "train_bool = smarket['Year'].values < 2005\n",
        "X_train = X[train_bool]\n",
        "X_test = X[~train_bool]\n",
        "y_train = y[train_bool]\n",
        "y_test = y[~train_bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp7Wco_Qke9B"
      },
      "outputs": [],
      "source": [
        "results = smf.logit('Up ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5', data=smarket).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCnbv4Wmke9B"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2FdqRlpke9B"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmw1LJvOke9B"
      },
      "outputs": [],
      "source": [
        "# true on the left axis, predicted above\n",
        "confusion_matrix(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28HOOzTJke9B"
      },
      "outputs": [],
      "source": [
        "147/ len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIDAxn8_ke9B"
      },
      "source": [
        "Out of the 68 predicted down, 37 actually were down days. 54% accurracy  \n",
        "Out of the 184 predicted up, 110 actually were up. 60% accuracy.  \n",
        "58% total accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aqEuoe9ke9B"
      },
      "outputs": [],
      "source": [
        "y_pred = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pviEo3vke9B"
      },
      "outputs": [],
      "source": [
        "y_pred[y_test == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaVOsSzske9B"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shLQvmwRke9C"
      },
      "outputs": [],
      "source": [
        "lda = LinearDiscriminantAnalysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTlgIDDkke9C"
      },
      "outputs": [],
      "source": [
        "lda.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZdU6OrEke9C"
      },
      "outputs": [],
      "source": [
        "#almost exact same as logistic regression\n",
        "confusion_matrix(y_test, lda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkEf5mr1ke9C"
      },
      "outputs": [],
      "source": [
        "lda.priors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wza3Onvnke9C"
      },
      "outputs": [],
      "source": [
        "# use QDA with only 2 variables\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train[:,:2], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bl-O0heke9C"
      },
      "outputs": [],
      "source": [
        "#almost exact same as logistic regression\n",
        "confusion_matrix(y_test, qda.predict(X_test[:,:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkP2hLZnke9C"
      },
      "outputs": [],
      "source": [
        "# knn\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iac8uJ-7ke9C"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCStyFREke9C"
      },
      "outputs": [],
      "source": [
        "knn.fit(X_train[:,:2], y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEuPPjV5ke9E"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test, knn.predict(X_test[:,:2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "muhLvhUike9E"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ofVv1Rke9F"
      },
      "source": [
        "# 1\n",
        "Turn equation 4.2 to 4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJta0ExZke9F"
      },
      "source": [
        "4.2\n",
        "$$p(x) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$$\n",
        "\n",
        "4.3\n",
        "$$\\frac{p(x)}{1 - p(x)} = e^{\\beta_0 + \\beta_1X}$$\n",
        "\n",
        "First, multiply 4.2 by negative 1 and add 1 to both sides\n",
        "$$1 - p(x) = 1 - \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$$\n",
        "Simplify right hand side\n",
        "$$1 - p(x) = \\frac{1}{1 + e^{\\beta_0 + \\beta_1X}}$$\n",
        "Now just divide 4.2 by the last equation and you have the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFL6eAEVke9F"
      },
      "source": [
        "# 2\n",
        "Prove that 4.13 maximizes 4.12.\n",
        "\n",
        "Since log is a monotone increasing function, maximizing the log of a function will also maximize the original function. After taking the log of 4.12 all constants can be dropped and you are left with the 4.13."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKUMFy6Fke9F"
      },
      "source": [
        "# 3\n",
        "Prove that QDA is quadratic with one predictor\n",
        "\n",
        "This is show by taking the log of 4.12 and throwing away all terms that don't depend on k. An extra term will be left attached to $x^2$ thus making it quadratic. This term disappears in LDA because all the variances are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9ni0Phke9F"
      },
      "source": [
        "# 4\n",
        "It's easiest to think in terms of each X in a range of 0 - 100\n",
        "a) 10 / 100 = 10%  \n",
        "b) 10 x 10 / (100 x 100) = 1%  \n",
        "c) $10^{-100}$. The fraction of nearest neighbors is $10^{-p}$  \n",
        "d) even in 2 dimensions only 1% of neighbors will be within 5% on either side  \n",
        "e) 10% for p=1.\n",
        "Generalizing we get $.1^{1/p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfyeInUake9F"
      },
      "source": [
        "# 5\n",
        "a) QDA on training. LDA on test  \n",
        "b) QDA on both  \n",
        "c) Since QDA is a quadratic model, more data should improve the model faster than LDA  \n",
        "d) False, QDA will overfit by finding a different variance for each class when in reality the variance for each class are the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOHvgdyRke9F"
      },
      "source": [
        "# 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj1GoP6wke9F"
      },
      "outputs": [],
      "source": [
        "# a\n",
        "b0 = -6\n",
        "b1 = .05\n",
        "b2 = 1\n",
        "x1 = 40\n",
        "x2 = 3.5\n",
        "t = -6 + b1 * x1 + b2 * x2\n",
        "print(\"student has a {:.3f} probability of getting an A\".format(1 / (1 + np.exp(-t))))\n",
        "\n",
        "#b. solve for t = 0. Since an odds of 1 corresponds to 50/50 chance and log(1) = 0\n",
        "# 0 = -6 + b1 * x1 + b2 * x2\n",
        "hours = (6 - b2 * x2) / b1\n",
        "print(\"student needs to study {} hours to have a 50% chance at an A\".format(1 / (1 + np.exp(-t))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVFwmjQIke9F"
      },
      "outputs": [],
      "source": [
        "# double check 50%\n",
        "b0 = -6\n",
        "b1 = .05\n",
        "b2 = 1\n",
        "x1 = 50\n",
        "x2 = 3.5\n",
        "t = -6 + b1 * x1 + b2 * x2\n",
        "1 / (1 + np.exp(-t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpVPjUTVke9F"
      },
      "source": [
        "# 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFDPzxnwke9F"
      },
      "outputs": [],
      "source": [
        "# 7\n",
        "prior = .8\n",
        "mu_d = 10\n",
        "mu_no_d = 0\n",
        "sigma = 6\n",
        "normal = lambda x, m, s: 1 / np.sqrt(2 * np.pi * s ** 2) * np.exp(-(x - m) ** 2 / (2 * s ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ocMauUfke9G"
      },
      "outputs": [],
      "source": [
        "f_d = normal(4, 10, 6)\n",
        "f_no_d = normal(4, 0, 6)\n",
        "f_d, f_no_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERVrZEXZke9G"
      },
      "outputs": [],
      "source": [
        "# bayes\n",
        "prob_div = prior * f_d / (prior * f_d + (1 - prior) * f_no_d)\n",
        "print(\"Probability of dividend is {:.3f}\".format(prob_div))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDsNPG6_ke9G"
      },
      "source": [
        "# 8\n",
        "Since knn with k equal to 1 always picks itself, its training error is 0. Meaning in the given example the test error is .36 and worse than logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scuBR7pGke9G"
      },
      "source": [
        "# 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKq8dfMPke9G"
      },
      "outputs": [],
      "source": [
        "# a\n",
        "# p / (1 - p) = .37\n",
        "# 1 / p - 1 = 1 / .37\n",
        "odds = .37\n",
        "one_over_p = 1 + 1 / odds\n",
        "p = 1 / one_over_p\n",
        "print(\"The probability of defaulting with odds of {} are {:.2f}\".format(odds, p))\n",
        "print(\"The odds of defaulting with probability .16 are {:.2f}\".format(.16 / .84))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsLgle5zke9G"
      },
      "source": [
        "# 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jygQ6g-ke9G"
      },
      "outputs": [],
      "source": [
        "weekly = pd.read_csv(\"data/weekly.csv\")\n",
        "weekly.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfcWnsUQke9G"
      },
      "outputs": [],
      "source": [
        "# strongest correlations with today are lag1 and lag3\n",
        "weekly.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN8vieDRke9G"
      },
      "outputs": [],
      "source": [
        "today = weekly['Today']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVx3Ina1ke9G"
      },
      "outputs": [],
      "source": [
        "today_perc = (100 + today) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn2p-BGeke9H"
      },
      "outputs": [],
      "source": [
        "today_perc.cumprod().plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96RWHCbkke9H"
      },
      "outputs": [],
      "source": [
        "weekly['Volume'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwo3BRqeke9H"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot('Direction', 'Lag1', data=weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIIFk52Ike9H"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot('Direction', 'Lag3', data=weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-f8-YaWke9H"
      },
      "outputs": [],
      "source": [
        "weekly['Direction'] = np.where(weekly['Direction'] == 'Up', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8AjWXOrke9H"
      },
      "outputs": [],
      "source": [
        "# wow lag2 is statistically significant\n",
        "results = smf.logit('Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data=weekly).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4OKtjmdke9H"
      },
      "outputs": [],
      "source": [
        "# wow lag2 is statistically significant\n",
        "results = smf.logit('Direction ~ Lag2', data=weekly).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6U5WYOhke9H"
      },
      "outputs": [],
      "source": [
        "predictions = np.where(results.predict(weekly) > .5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aauxAhenke9H"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(weekly['Direction'], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_xNijTFke9H"
      },
      "outputs": [],
      "source": [
        "451 / 1030"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fu4FUYFke9I"
      },
      "outputs": [],
      "source": [
        "weekly['Direction'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dTjFTjke9I"
      },
      "source": [
        "The default prediciton is Up - about 95% of the predictions are up and it actually gets 56.2% correct but that's only slightly higher than the 55.6% total up days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0cj9IFKke9I"
      },
      "outputs": [],
      "source": [
        "# use sklearn for rest of problems\n",
        "year_bool = weekly['Year'] < 2009\n",
        "weekly['ones'] = 1\n",
        "X_train = weekly[year_bool][['ones', 'Lag2']].values\n",
        "X_test = weekly[~year_bool][['ones', 'Lag2']].values\n",
        "y_train = weekly[year_bool]['Direction'].values\n",
        "y_test = weekly[~year_bool]['Direction'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wRzNsYyke9I"
      },
      "outputs": [],
      "source": [
        "lr =  LogisticRegression()\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJrCAr3Pke9I"
      },
      "outputs": [],
      "source": [
        "# not bad\n",
        "confusion_matrix(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0GhId3xke9I"
      },
      "outputs": [],
      "source": [
        "# e\n",
        "# use LDA\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train)\n",
        "confusion_matrix(y_test, lda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B96X_Tkpke9I"
      },
      "outputs": [],
      "source": [
        "# terrible\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train)\n",
        "confusion_matrix(y_test, qda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz34sO_Mke9I"
      },
      "outputs": [],
      "source": [
        "# very poor\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "confusion_matrix(y_test, knn.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iilm5M2kke9I"
      },
      "outputs": [],
      "source": [
        "# h\n",
        "# logistic regression and LDA work the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHbGhnoMke9I"
      },
      "outputs": [],
      "source": [
        "# i\n",
        "# not bad when predicting up\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train, y_train)\n",
        "confusion_matrix(y_test, knn.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yDLZKZWke9I"
      },
      "outputs": [],
      "source": [
        "# try many combinations\n",
        "results = smf.logit('Direction ~ np.power(Lag5, 2)', data=weekly).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWUlOBKske9J"
      },
      "outputs": [],
      "source": [
        "# try many combinations\n",
        "results = smf.logit('Direction ~ np.power(Volume, 2)', data=weekly).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLj12RsBke9J"
      },
      "outputs": [],
      "source": [
        "# try many combinations\n",
        "results = smf.logit('Direction ~ Volume * Lag3', data=weekly).fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDXyuBh0ke9J"
      },
      "source": [
        "# 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt9aKHU0ke9J"
      },
      "outputs": [],
      "source": [
        "# a\n",
        "auto = pd.read_csv('data/auto.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soGblmJ6ke9J"
      },
      "outputs": [],
      "source": [
        "auto['mpg01'] = np.where(auto['mpg'] > auto['mpg'].median(), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ERhiSDfke9J"
      },
      "outputs": [],
      "source": [
        "auto.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Io_N14ke9J"
      },
      "outputs": [],
      "source": [
        "X = auto[['cylinders', 'origin']].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7yqHjHZke9J"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LsHS8ZIke9J"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "accuracy_score(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KNV7Sxtke9J"
      },
      "outputs": [],
      "source": [
        "X = auto[['cylinders', 'origin', 'year', 'acceleration']].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8Yv1hAVke9J"
      },
      "outputs": [],
      "source": [
        "# slightly higher with more variables\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "accuracy_score(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7dXUeuKke9J"
      },
      "outputs": [],
      "source": [
        "X = auto[auto.columns[1:-1] - ['name']].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "# Quite a bit better with all variables and high regularlization\n",
        "lr = LogisticRegression(C=.01)\n",
        "lr.fit(X_train, y_train)\n",
        "accuracy_score(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bxz_9C7ke9K"
      },
      "outputs": [],
      "source": [
        "X = auto[auto.columns[1:-1].difference(['name'])].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train)\n",
        "accuracy_score(y_test, lda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6yztJeke9K"
      },
      "outputs": [],
      "source": [
        "X = auto[auto.columns[1:-1].difference(['name'])].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train)\n",
        "accuracy_score(y_test, qda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDCwH85jke9K"
      },
      "outputs": [],
      "source": [
        "X = auto[auto.columns[1:-1].difference(['name'])].values\n",
        "y = auto['mpg01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLIlb_yzke9K"
      },
      "outputs": [],
      "source": [
        "# looks like 18-32 for K yields the best results\n",
        "for k in range(1, 51):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "    print('With K={} accuracy is {:.3f}'.format(k, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1bMOFSoke9K"
      },
      "source": [
        "# 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYv17Hpoke9K"
      },
      "outputs": [],
      "source": [
        "power = lambda x, a: x ** a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbN03QcIke9K"
      },
      "outputs": [],
      "source": [
        "power(3, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0iu9U-Rke9K"
      },
      "outputs": [],
      "source": [
        "n = 100\n",
        "plt.plot(range(n), [power(x, 2) for x in range(n)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-T9RCiGke9L"
      },
      "outputs": [],
      "source": [
        "def plot_power(rng, p):\n",
        "    plt.plot(rng, [power(x, p) for x in rng])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK_Xavaike9L"
      },
      "outputs": [],
      "source": [
        "plot_power(range(3,14), 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwEW7Rokke9L"
      },
      "source": [
        "# 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCpfkbioke9L"
      },
      "outputs": [],
      "source": [
        "boston = pd.read_csv('data/boston.csv')\n",
        "boston['crim01'] = np.where(boston['crim'] > boston['crim'].median(), 1, 0)\n",
        "boston.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMw08IYvke9L"
      },
      "outputs": [],
      "source": [
        "X = boston.iloc[:,1:-1].values\n",
        "y = boston['crim01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train)\n",
        "accuracy_score(y_test, qda.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vQhXJb1ke9L"
      },
      "outputs": [],
      "source": [
        "X = boston.iloc[:,1:-1].values\n",
        "y = boston['crim01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "accuracy_score(y_test, knn.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huyep6xUke9L"
      },
      "outputs": [],
      "source": [
        "X = boston.iloc[:,1:-1].values\n",
        "y = boston['crim01'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "\n",
        "lr = LogisticRegression(C=1)\n",
        "lr.fit(X_train, y_train)\n",
        "accuracy_score(y_test, lr.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgt_pmgSke9L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}